<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Our Data, Their Profit | Home</title>
  <link rel="stylesheet" href="default.css">
  <!-- Google Fonts: Decorative header font -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Grape+Nuts&display=swap" rel="stylesheet">
  <!-- Font Awesome: Icon library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body>
  <!-- Header with site title and navigation -->
  <header>
    <h1>The Age of Surveillance Capitalism</h1>
    <!-- Navigation menu -->
    <nav>
      <a href="index.html">Home</a>
      <a href="what-it-is.html">What It Is</a>
      <a href="behavioral-surplus.html">Behavioral Surplus</a>
      <a href="how-they-track-you.html">How They Track You</a>
      <a href="why-it-matters.html">Why It Matters</a>
      <a href="who-profits.html">Who Profits</a>
      <a href="real-world-consequences.html">Real world Consequences</a>
      <a href="reclaim-digital-autonomy.html">Reclaim Digital Autonomy</a>
      <a href="ethical-legal.html">Ethical & Legal</a>
      <a href="resources-credits.html">Resources & Credits</a>
    </nav>
  </header>

  <!-- Main content container -->
  <div class="container">
    <!-- Page title -->
    <h2>Real World Consequences</h2>
    <!-- Page introduction -->
    <p>Surveillance capitalism isn't theoretical ‚Äî it has already caused real harm to real people. Here are documented cases where data extraction led to manipulation, discrimination, and abuse.</p>

    <!-- Warning banner explaining the severity -->
    <div class="warning-banner">
      <p>‚ö†Ô∏è <strong>Content Warning:</strong> The following cases involve discrimination, manipulation, and harm. They are included to demonstrate the real-world impact of surveillance capitalism.</p>
    </div>

    <!-- Case Study 1: Cambridge Analytica -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>Cambridge Analytica: Election Manipulation</h3>
      <p><strong>What Happened:</strong> Political consulting firm Cambridge Analytica obtained behavioral data on millions of Facebook users (without their consent) and used it to create psychological profiles.</p>
      <p><strong>The Data:</strong> They combined Facebook behavioral data with consumer data, financial information, and social media activity to build detailed psychological profiles on voters.</p>
      <p><strong>The Manipulation:</strong> Armed with this data, they created thousands of customized political advertisements targeting specific psychological vulnerabilities. One voter might see messages about immigration, another about healthcare ‚Äî all designed to manipulate their emotions and voting behavior.</p>
      <p><strong>The Impact:</strong> Used in the 2016 US Presidential election, Brexit campaign, and numerous other elections worldwide. Demonstrated that elections could be influenced through targeted data manipulation.</p>
      <p><strong>Lesson:</strong> Your behavioral data is powerful enough to swing elections. When combined with psychological profiling, it becomes a tool for mass manipulation.</p>
    </div>

    <!-- Case Study 2: Clearview AI -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>Clearview AI: Mass Facial Recognition Database</h3>
      <p><strong>What Happened:</strong> Clearview AI scraped billions of photos from social media, dating apps, and other websites to build a facial recognition database without consent.</p>
      <p><strong>The Data:</strong> Over 30 billion photos from Facebook, YouTube, Twitter, Instagram, dating apps, and mugshot databases combined into one massive facial recognition system.</p>
      <p><strong>The Access:</strong> Sold access to law enforcement, immigration agencies, and private companies. Police could upload a photo of anyone and instantly find all their social media profiles and other information.</p>
      <p><strong>The Impact:</strong> Innocent people were arrested based on faulty facial recognition matches. Protesters were tracked and identified. Immigration enforcement used it to target undocumented immigrants and their families.</p>
      <p><strong>Lesson:</strong> Your photos are being used to build surveillance systems you didn't consent to. Facial recognition combined with behavioral data creates unprecedented tracking capabilities.</p>
    </div>

    <!-- Case Study 3: Amazon Ring -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>Amazon Ring: Neighborhood Surveillance Network</h3>
      <p><strong>What Happened:</strong> Amazon's Ring doorbell cameras are creating a crowdsourced surveillance network. Police departments can request footage from neighbors' cameras, and homeowners unknowingly participate in tracking people.</p>
      <p><strong>The Data:</strong> Real-time video of everyone entering/leaving homes, faces, vehicles, license plates, and movement patterns throughout neighborhoods.</p>
      <p><strong>The Surveillance:</strong> Police access footage without warrants. Amazon shares data with law enforcement. Innocent people are recorded and tracked in their own neighborhoods.</p>
      <p><strong>The Impact:</strong> Reduced privacy in public spaces. Racial profiling amplified through automated surveillance. Protesters and activists tracked. Abusive partners using Ring data to monitor ex-partners.</p>
      <p><strong>Lesson:</strong> Surveillance capitalism extends beyond the internet. Physical world tracking combines with digital data to create inescapable surveillance.</p>
    </div>

    <!-- Case Study 4: Target Pregnancy Prediction -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>Target: Predicting Pregnancy Before You Tell Anyone</h3>
      <p><strong>What Happened:</strong> Target used behavioral surplus to predict which customers were pregnant based on purchasing patterns.</p>
      <p><strong>The Data:</strong> Purchases of unscented lotion, vitamin supplements, cotton balls, and other products were analyzed to identify pregnant women.</p>
      <p><strong>The Targeting:</strong> Pregnant women received targeted promotions for baby products before they'd told friends, family, or sometimes even their partners.</p>
      <p><strong>The Impact:</strong> One father discovered his teenage daughter was pregnant through a Target ad sent to his home. People's most private moments were monetized without their knowledge.</p>
      <p><strong>Lesson:</strong> Companies know intimate details about your life from behavioral surplus. This data is used to target you at vulnerable moments.</p>
    </div>

    <!-- Section: Discrimination Cases -->
    <h3 class="section-title">Algorithmic Discrimination</h3>

    <!-- Discrimination Case 1: Criminal Justice -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>COMPAS: Racist Sentencing Algorithm</h3>
      <p><strong>What Happened:</strong> Courts used the COMPAS algorithm to predict recidivism risk and inform sentencing decisions.</p>
      <p><strong>The Bias:</strong> The algorithm was trained on historical criminal justice data that reflects systemic racism. It learned to associate race with criminal risk.</p>
      <p><strong>The Results:</strong> Black defendants were labeled "high risk" at twice the rate of white defendants. They received longer sentences based on algorithmic predictions trained on racist historical data.</p>
      <p><strong>Impact:</strong> Innocent people received harsher sentences. Systemic racism was mathematized and made to seem objective.</p>
    </div>

    <!-- Discrimination Case 2: Hiring -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>Amazon's Biased Hiring Algorithm</h3>
      <p><strong>What Happened:</strong> Amazon developed an AI recruiting tool that was supposed to help screen job candidates faster.</p>
      <p><strong>The Bias:</strong> Trained on 10 years of historical hiring data from a male-dominated tech industry, the algorithm learned to discriminate against women.</p>
      <p><strong>The Results:</strong> Women's resumes were downranked. Female candidates were filtered out before humans even saw them. The algorithm had learned that men were "better" candidates because that's who had been hired historically.</p>
      <p><strong>Impact:</strong> Systemic discrimination against women in hiring, made invisible by "objective" algorithms.</p>
    </div>

    <!-- Section: Social and Mental Health Impact -->
    <h3 class="section-title">Social & Mental Health Consequences</h3>

    <!-- Mental Health Impact Card -->
    <div class="impact-card">
      <div class="impact-icon">üò∞</div>
      <h3>Anxiety and Depression in Youth</h3>
      <p>Teen suicide rates, anxiety, and depression have spiked in correlation with social media adoption. Algorithms optimize for engagement by showing content that triggers emotional reactions ‚Äî anger, fear, and anxiety are highly engaging.</p>
      <p>Your behavioral data shows what upsets you, what triggers anxiety, and what keeps you scrolling. Algorithms use this to maximize your time on platform, knowing it's harming your mental health.</p>
    </div>

    <!-- Misinformation Impact Card -->
    <div class="impact-card">
      <div class="impact-icon">ü§•</div>
      <h3>Misinformation and Filter Bubbles</h3>
      <p>Algorithmic feeds personalized on behavioral data create "filter bubbles" ‚Äî you only see content that confirms your existing beliefs. Misinformation spreads faster than truth because emotional (false) content is more engaging.</p>
      <p>Your behavioral surplus determines what reality you see. Different people live in different information universes, making shared truth impossible.</p>
    </div>

    <!-- Addiction Impact Card -->
    <div class="impact-card">
      <div class="impact-icon">üì±</div>
      <h3>Behavioral Addiction</h3>
      <p>Tech platforms are deliberately designed to be addictive. Former Facebook and Google engineers have admitted to engineering addiction features.</p>
      <p>Notifications, infinite scroll, and algorithmic feeds are all designed using behavioral surplus insights to maximize screen time and create dependency.</p>
    </div>

    <!-- Section: Economic Harm -->
    <h3 class="section-title">Economic Harm</h3>

    <!-- Predatory Lending Case -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>Predatory Lending: Data-Driven Exploitation</h3>
      <p><strong>What Happened:</strong> Payday lending companies use behavioral data to identify vulnerable people in financial distress.</p>
      <p><strong>The Data:</strong> They track people who search for "quick loans," have late bill payments, and show other signs of financial distress.</p>
      <p><strong>The Targeting:</strong> Aggressive ads targeted at desperate people offering loans with 400%+ APR interest rates.</p>
      <p><strong>The Impact:</strong> Low-income individuals trapped in debt cycles. Behavioral surplus is used to identify and exploit the most vulnerable people.</p>
    </div>

    <!-- Discriminatory Pricing -->
    <div class="case-study">
      <span class="case-label">CASE STUDY</span>
      <h3>Dynamic Pricing: You Pay More Based on Your Data</h3>
      <p><strong>What Happened:</strong> Online retailers use behavioral data to show different prices to different people for the same product.</p>
      <p><strong>How It Works:</strong> If your data shows you have high income, you see a higher price. If your browsing history shows you're desperate to buy, the price goes up. If you're on mobile (suggesting you're on-the-go and less price-sensitive), you pay more.</p>
      <p><strong>The Impact:</strong> Same product, different prices. Those with less money pay more because companies know they're less price-sensitive or less likely to comparison shop.</p>
    </div>

    <!-- Privacy warning: Summary of Harms -->
    <div class="privacy-warning">
      <h3>The Common Thread</h3>
      <p>These cases share a pattern: <strong>Behavioral surplus is used to target the most vulnerable, manipulate people at scale, and amplify existing inequalities.</strong></p>
      <p>Your data is weaponized against you. The more data companies have, the better they can exploit you.</p>
    </div>

    <!-- Call to action -->
    <h3 class="section-title">What Can You Do?</h3>
    <p>These harms are serious, but not inevitable. Visit the <a href="reclaim-digital-autonomy.html">Reclaim Your Digital Autonomy</a> page for concrete steps you can take to protect yourself and advocate for change.</p>

  </div>

  <!-- Footer with copyright -->
  <footer>
    <small>
      <p>¬© 2025 project for Cart 211 | Built for educational use</p>
    </small>
  </footer>
</body>
</html>